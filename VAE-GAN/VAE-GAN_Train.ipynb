{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5452b0-fa0b-4df8-a26e-ebd77115d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4839d5-d3e5-456c-ad27-e8b5b2494709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary public libraries as well as classes and functions written by the author\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "from __utils.functions import calc_MSE, calc_SSIM, find_most_similar_img\n",
    "from __utils.functions import Imagefolder\n",
    "from __utils.functions import show_img, save_tensor_as_img, img_to_grid_save\n",
    "from __utils.functions import weight_init, reset_grad\n",
    "from __utils.functions import plot_line_graph\n",
    "from __utils.functions import find_key\n",
    "from __models.VAEGAN import Discriminator, VariationalAutoEncoder\n",
    "sys.path.remove(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1d639d-3fc5-4706-beca-2d9e0a5c54da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU rather than CPU to accelerate the training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558f01a-9c0d-4c19-a789-7d850f4852ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hp_lr = 3e-4\n",
    "hp_batch_size = 32\n",
    "hp_epochs = 20\n",
    "hp_latent_dims = 32\n",
    "hp_gamma_d = 15 \n",
    "hp_gamma_e = 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92258664-3f23-4738-aaaf-f7ff46b1aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move GAN to GPU\n",
    "vae = VariationalAutoEncoder(latent_dims=hp_latent_dims).to(device)\n",
    "netD = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09386a-2dc4-403b-9ea7-034857f7293b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see the detail of GAN by runing this block\n",
    "print(summary(netD, (1,128,128)))\n",
    "print(summary(vae, (1,128,128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659d534-5b0c-4c96-af92-d91cc43206da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained parameters (For fune-tuning only)\n",
    "# netD.load_state_dict(torch.load(''))\n",
    "# vae.load_state_dict(torch.load(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd38e41-9987-4c2b-899e-4c0b60367c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the loss after each iteration\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "E_losses = []\n",
    "# Store the SSIM after each iteration\n",
    "SSIM_history = []\n",
    "# This dict stores the classes of SAR images and their corresponding values\n",
    "class_dict = {'2S1': 0,\n",
    "             'BMP2': 1,\n",
    "             'BRDM2': 2,\n",
    "             'BTR60': 3,\n",
    "             'BTR70': 4,\n",
    "             'D7': 5,\n",
    "             'T62': 6,\n",
    "             'T72': 7,\n",
    "             'ZIL131': 8,\n",
    "             'ZSU234': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1fc1d-463f-4042-8489-586b39c591e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary-CrossEntropy Loss is to calculate the loss function of a standard GAN\n",
    "loss = nn.BCELoss()\n",
    "# Specify the value of labels to help netD\n",
    "real_label = 1.0\n",
    "fake_label = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfc4ada-5f90-4902-bf7c-38369c1de3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for classes in range(10):\n",
    "    # Clear output after each loop\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Define the optimizer of GAN\n",
    "    vae = VariationalAutoEncoder(latent_dims=hp_latent_dims).to(device)\n",
    "    netD = Discriminator().to(device)\n",
    "    solver_E = optim.Adam(vae.encoder.parameters(), lr=hp_lr)\n",
    "    solver_D = optim.Adam(vae.decoder.parameters(), lr=hp_lr)\n",
    "    solver_Dis = optim.Adam(netD.parameters(), lr=hp_lr/3)\n",
    "\n",
    "    # Initilize the weight of GAN\n",
    "    vae = vae.apply(weight_init)\n",
    "    netD = netD.apply(weight_init)\n",
    "    \n",
    "    category_name = find_key(class_dict, classes)\n",
    "    \n",
    "    # Load the training data\n",
    "    dataset1, _ = Imagefolder(root='../_MSTAR/TRAIN/', normalize=True, category_idx=classes)\n",
    "    dataloader_1 = DataLoader(dataset1, batch_size=hp_batch_size, shuffle=True, drop_last=True)\n",
    "    # validation data\n",
    "    dataset2, _ = Imagefolder(root='../_MSTAR/TEST/', normalize=True, category_idx=classes) \n",
    "    dataloader_2 = DataLoader(dataset2, batch_size=16, shuffle=True)\n",
    "\n",
    "    print(\"——————————Now start training————————\")\n",
    "\n",
    "    for epoch in range(hp_epochs):\n",
    "        for i, (img, _) in enumerate(dataloader_1):\n",
    "            vae.train()\n",
    "            netD.train()\n",
    "\n",
    "            real_label = torch.ones(hp_batch_size, 1).cuda()\n",
    "            fake_label = torch.zeros(hp_batch_size, 1).cuda()\n",
    "            img = img.cuda()\n",
    "            mean, logvar, recon_img = vae(img)\n",
    "            noise = torch.randn(hp_batch_size, hp_latent_dims).cuda()\n",
    "            sampled_img = vae.decoder(noise)\n",
    "\n",
    "            #############   ===================   #############\n",
    "            #############   Train Discriminator   #############\n",
    "            #############   ===================   #############\n",
    "            solver_Dis.zero_grad()\n",
    "\n",
    "            dis_origin_result, dis_origin_latent = netD(img)\n",
    "            dis_recon_result, dis_recon_latent = netD(recon_img)\n",
    "            dis_sampled_result, dis_sampled_latent = netD(sampled_img)\n",
    "\n",
    "            err_dis_origin = loss(dis_origin_result, real_label)\n",
    "            err_dis_recon = loss(dis_recon_result, fake_label)\n",
    "            err_dis_sampled = loss(dis_sampled_result, fake_label)\n",
    "            dis_loss = err_dis_origin + err_dis_recon + err_dis_sampled\n",
    "\n",
    "            dis_loss.backward(retain_graph=True)\n",
    "            solver_Dis.step()\n",
    "\n",
    "            #############   =============   #############\n",
    "            #############   Train Decoder   #############\n",
    "            #############   =============   #############\n",
    "            # with torch.autograd.set_detect_anomaly(True):\n",
    "            solver_D.zero_grad()\n",
    "\n",
    "            dis_origin_result, dis_origin_latent = netD(img)\n",
    "            dis_recon_result, dis_recon_latent = netD(recon_img)\n",
    "            dis_sampled_result, dis_sampled_latent = netD(sampled_img)\n",
    "\n",
    "            err_dis_origin = loss(dis_origin_result, real_label)\n",
    "            err_dis_recon = loss(dis_recon_result, fake_label)\n",
    "            err_dis_sampled = loss(dis_sampled_result, fake_label)\n",
    "\n",
    "            d_loss_1 = err_dis_recon + 1*err_dis_sampled\n",
    "            d_loss_2 = torch.mean((dis_origin_latent - dis_recon_latent)**2)\n",
    "            d_loss = hp_gamma_d*d_loss_2 - d_loss_1\n",
    "\n",
    "            d_loss.backward()\n",
    "            solver_D.step()\n",
    "\n",
    "            #############   =============   #############\n",
    "            #############   Train Encoder   #############\n",
    "            #############   =============   #############\n",
    "            solver_E.zero_grad()\n",
    "\n",
    "            mean, logvar, recon_img = vae(img)\n",
    "            _, dis_origin_latent = netD(img)\n",
    "            _, dis_recon_latent = netD(recon_img)\n",
    "            e_loss_1 = torch.mean((dis_origin_latent - dis_recon_latent)**2)\n",
    "            e_loss_2 = (-0.5 * torch.sum(-logvar.exp() - torch.pow(mean,2) + logvar + 1)) / torch.numel(mean.data)\n",
    "            e_loss = hp_gamma_e*e_loss_1 + e_loss_2\n",
    "\n",
    "            e_loss.backward()\n",
    "            solver_E.step()\n",
    "\n",
    "    #         Print loss during training, easy to track the performance\n",
    "            if i % 100 == 0:\n",
    "                print('{%d/10}|\"%s\"|[%d/%d](%d/%d)\\tLoss_Dis: %.4f\\tLoss_E: %.4f\\tLoss_D: %.4f'\n",
    "                      % (classes+1, category_name, epoch+1, hp_epochs, i, len(dataloader_1),\n",
    "                         dis_loss.mean().item(), e_loss.mean().item(), d_loss.mean().item()))\n",
    "                \n",
    "    #         Store the loss in two lists\n",
    "            if i == 5:\n",
    "                D_losses.append(dis_loss.mean().item())\n",
    "                G_losses.append(d_loss.mean().item())\n",
    "                E_losses.append(e_loss.mean().item())\n",
    "\n",
    "    #     Show the generated images after 10 epoch\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            vae.eval()\n",
    "            with torch.no_grad():\n",
    "                noise = torch.randn(16, hp_latent_dims).cuda()\n",
    "                generated_samples = vae.decoder(noise).cpu()\n",
    "                show_img(generated_samples[0:9], normalize=True, dpi=100, title='Generated Samples')\n",
    "                img_test, _ = next(iter(dataloader_2))\n",
    "                img_test = img_test.cuda()\n",
    "                _, _, recon_img_test = vae(img_test)\n",
    "            show_img(img[0:16], normalize=True, dpi=100, title='Original Train Images')\n",
    "            show_img(recon_img[0:16], normalize=True, dpi=100, title='Reconstructed Train Images')\n",
    "            show_img(img_test, normalize=True, dpi=100, title='Original Test Images')\n",
    "            show_img(recon_img_test, normalize=True, dpi=100, title='Reconstructed Test Images')\n",
    "            grid_1 = make_grid(img[0:16], cmap='gray', normalize=True, nrow=4)\n",
    "            grid_2 = make_grid(recon_img[0:16], cmap='gray', normalize=True, nrow=4)\n",
    "            grid_3 = make_grid(img_test, cmap='gray', normalize=True, nrow=4)\n",
    "            grid_4 = make_grid(recon_img_test, cmap='gray', normalize=True, nrow=4)\n",
    "            big_grid = make_grid([grid_1, grid_2, grid_3, grid_4], cmap='gray', normalize=False, nrow=2, pad_value=32)\n",
    "            root = ('history/%s/' %category_name)\n",
    "            if not os.path.exists(root):\n",
    "                os.makedirs(root)\n",
    "            save_image(big_grid, root+'Epoch_compare_%d.png' %(epoch+1))\n",
    "            img_to_grid_save(generated_samples[0:9], root='history/%s/' %category_name, name='Epoch_sampling_%d' %(epoch+1), name_num=False)\n",
    "            \n",
    "    # Save the parameters of the GAN\n",
    "    torch.save(vae.state_dict(), 'VariationalAutoEncoder_%s.pkl' %category_name)\n",
    "    torch.save(netD.state_dict(), 'Discriminator_%s.pkl' %category_name)\n",
    "\n",
    "#     Save Reconstructed Images\n",
    "    dataloader_1 = DataLoader(dataset1, batch_size=10, shuffle=False, drop_last=False)\n",
    "    for i, (img,_) in enumerate(dataloader_1):\n",
    "        vae.eval()\n",
    "        for j in range(5):\n",
    "            _, _, recon_img = vae(img.cuda())\n",
    "            save_tensor_as_img(recon_img, root='images/VAEGAN_recon/%s' %category_name, fmt='gray', normalize=True, name='%d_%d' %(i,j))\n",
    "#     Save Sampled Images     \n",
    "    for j in range(5):\n",
    "        noise = torch.randn(200, hp_latent_dims).cuda()\n",
    "        generated_img = vae.decoder(noise)\n",
    "        save_tensor_as_img(generated_img, root='images/VAEGAN_sample/%s' %category_name, fmt='gray', normalize=True, name='%s_%d' %(category_name,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c77cf2-4e2a-4221-abb4-e94ca232f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Loss Progess of Discriminator and Decoder(Generator)\n",
    "plot_line_graph(line1=D_losses, name1='Dis', line2=G_losses, name2='Gen', dpi=100, title='Loss progress history', xlabel='epoch', ylabel='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d682e-52c9-4964-86c4-e4be7cbb08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Loss Progess of Encoder\n",
    "plot_line_graph(line1=E_losses, title='Encoder loss history', xlabel='epoch', ylabel='', dpi=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
